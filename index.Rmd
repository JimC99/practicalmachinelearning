---
title: "Practical Machine Learning Course Project"
author: "JC99"
date: "April 29, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Executive Summary
This report describes the development of a model for predicting the manner in which an exercise was performed by test subjects, using data recorded by personal devices worn by the subjects while they performed the exercise. The subjects were asked to perform the exercise - a single-arm dumbell curl - either correctly (class "A"), or else incorrectly in one of four specified manners (classes "B" through "E"). Using data from a subset of the recorded variables generated by the worn devices, a random forest model was developed that was 99.39% accurate in predicting the classe of exercise in a 40% cross-validation testing set.

## Data Preparation
Examination of data file "pml-training.csv" revealed that some variables (columns) appeared to have entries for most or all of the data points (rows), while other variables had numerous rows that were either blank, or contained not-useful "NA" or "#DIV/O" entries. Taking advantage of commonalities among the names of these columns, the following code was used to create subsets of the provided training and testing datasets that contained only the former, i.e., variables that were available for all data points. 
```{r subsetting, include = TRUE}
library(caret)
library(dplyr)
set.seed(10)
pml_training <- read.csv("pml-training.csv")
pml_testing <- read.csv("pml-testing.csv")
# Select variables whose names contain key fragments
pml_training.sub1 <- pml_training[,grepl("roll|pitch|yaw|gyros|accel|magnet|classe", colnames(pml_training))]
pml_testing.sub1 <- pml_testing[,grepl("roll|pitch|yaw|gyros|accel|magnet|classe", colnames(pml_testing))]
# Remove variables with mostly "NA"'s based on other key name fragments
pml_training.sub1 <- pml_training.sub1[,-grep("var|stddev|avg|min|max|amplitude|skewness|kurtosis", colnames(pml_training.sub1))]
pml_testing.sub1 <- pml_testing.sub1[,-grep("var|stddev|avg|min|max|amplitude|skewness|kurtosis", colnames(pml_testing.sub1))]
```
With pml-training and pml-testing thus reduced to 52 potential predictive variables, pml_training was next partitioned, using a 60:40 split, into training and testing subsets for the purpose of model generation.
```{r data partition, echo=TRUE, cache=TRUE}
# Separate pml_training.sub1 into training and testing sets
inTrain <- createDataPartition(y=pml_training.sub1$classe, p=0.6, list=FALSE)
training <- pml_training.sub1[inTrain,]
testing <- pml_training.sub1[-inTrain,]
```
## Model Generation
Linear discriminant analysis, random forest, and boosted trees models were each generated, then used to predict classe as a function of the variables in the training set. A fourth stacked model was also created from each of the other three.
```{r model generation, echo=TRUE, results="hide", cache=TRUE}
# Generate models using the training data
modlda <- train(classe~., method="lda", data=training)
modlGBM <- train(classe~., method="gbm", data=training)
modlRf <- train(classe~., method="rf", data=training)
# Predict classe in the testing set
predlda <- predict(modlda, testing)
predGBM <- predict(modlGBM, testing)
predRf <- predict(modlRf, testing)
# Generate combined model
predDF <- data.frame(predlda, predGBM, predRf, testing$classe)
# Simplify name of classe column 
colnames(predDF)[colnames(predDF) == "testing.classe"] <- "classe"
# Generate combo model and use it to predict classe
combModFit <- train(classe ~., method = "rf", data = predDF)
combpred <- predict(combModFit, predDF)
```

## Model Evaluation
To evaluate how well each model did, confusion matrices were generated from the results of each.
```{r model evaluation, echo=TRUE}
confusionMatrix(predlda, testing$classe)
confusionMatrix(predGBM, testing$classe)
confusionMatrix(predRf, testing$classe)
confusionMatrix(combpred, testing$classe)
```
## Model Comparison
Examination of the confusion matrices reveals that the combination model's results are identical to those of the random forest model. Thus the model chosen for analyzing test data in this report is the random forest model. This model had 99.39% accuracy on the test sample, or a 100-99.39% = 0.61% out-of-sample error rate. Threfore the expected out-of-sample error rate is 0.61%.